---
phase: 08-campaign-entity-writer
plan: "02"
type: execute
wave: 1
depends_on: []
files_modified:
  - prisma/schema.prisma
  - src/lib/knowledge/store.ts
  - src/lib/agents/writer.ts
  - src/lib/agents/orchestrator.ts
  - src/lib/agents/leads.ts
autonomous: true
requirements:
  - WRITER-08
  - WRITER-09

must_haves:
  truths:
    - "Knowledge base search uses pgvector cosine similarity instead of keyword matching"
    - "All existing 46+ documents are re-embedded during the migration"
    - "searchKnowledgeBase tool is available in the writer agent, leads agent, and orchestrator as a shared utility"
    - "Semantic search returns relevant results for conceptual queries that keyword matching would miss"
  artifacts:
    - path: "prisma/schema.prisma"
      provides: "KnowledgeChunk model with pgvector embedding column"
      contains: "model KnowledgeChunk"
    - path: "src/lib/knowledge/store.ts"
      provides: "searchKnowledge function using pgvector cosine similarity"
      contains: "cosine_similarity\\|<=>\\|vector"
    - path: "src/lib/knowledge/embeddings.ts"
      provides: "embedText function for generating embeddings"
      contains: "embedText"
  key_links:
    - from: "src/lib/knowledge/store.ts"
      to: "src/lib/knowledge/embeddings.ts"
      via: "embedText() call for query embedding"
      pattern: "embedText"
    - from: "src/lib/agents/writer.ts"
      to: "src/lib/knowledge/store.ts"
      via: "searchKnowledge import"
      pattern: "searchKnowledge"
---

<objective>
Upgrade knowledge base from keyword matching to pgvector semantic search and make searchKnowledgeBase a shared tool available to all agents.

Purpose: Semantic search enables the writer agent to find relevant best practices even when queries use different vocabulary than the stored documents. Making it shared lets all agents access the knowledge base.
Output: pgvector-backed knowledge search, re-embedded documents, shared tool across agents.
</objective>

<execution_context>
@/Users/jjay/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jjay/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-campaign-entity-writer/08-CONTEXT.md

<interfaces>
<!-- Current knowledge base interface -->

From src/lib/knowledge/store.ts:
```typescript
export function chunkText(text: string): string[]
export async function ingestDocument(options: {
  title: string; content: string; source: "upload" | "url"; tags?: string;
}): Promise<{ id: string; chunkCount: number }>
export async function searchKnowledge(
  query: string,
  options?: { limit?: number; tags?: string },
): Promise<{ title: string; chunk: string; tags: string | null }[]>
export async function listDocuments(): Promise<...>
export async function deleteDocument(id: string): Promise<void>
```

From prisma/schema.prisma:
```prisma
model KnowledgeDocument {
  id        String   @id @default(cuid())
  title     String
  source    String // "upload" | "url"
  content   String // Full original text
  chunks    String // JSON array of text chunks
  tags      String? // Comma-separated tags for filtering
  createdAt DateTime @default(now())
  @@index([source])
}
```

From src/lib/agents/writer.ts (searchKnowledgeBase tool, lines 121-157):
```typescript
searchKnowledgeBase: tool({
  description: "Search the knowledge base for cold email and LinkedIn outreach best practices...",
  inputSchema: z.object({
    query: z.string(),
    tags: z.string().optional(),
    limit: z.number().optional().default(8),
  }),
  execute: async ({ query, tags, limit }) => {
    const results = await searchKnowledge(query, { limit, tags });
    ...
  },
})
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pgvector extension and KnowledgeChunk model + embeddings utility</name>
  <files>prisma/schema.prisma, src/lib/knowledge/embeddings.ts</files>
  <action>
**Step 1: Enable pgvector in Prisma schema.**

Neon supports pgvector natively. Add the `pgvector` preview feature and the vector extension to prisma/schema.prisma:

Update the generator block:
```prisma
generator client {
  provider        = "prisma-client-js"
  previewFeatures = ["postgresqlExtensions"]
}
```

Update the datasource block:
```prisma
datasource db {
  provider   = "postgresql"
  url        = env("DATABASE_URL")
  extensions = [vector]
}
```

**Step 2: Add KnowledgeChunk model.**

Add after KnowledgeDocument:
```prisma
model KnowledgeChunk {
  id          String   @id @default(cuid())
  documentId  String
  content     String   // The chunk text
  embedding   Unsupported("vector(1536)")? // OpenAI text-embedding-3-small dimension
  chunkIndex  Int      // Position within the document
  createdAt   DateTime @default(now())

  document KnowledgeDocument @relation(fields: [documentId], references: [id], onDelete: Cascade)

  @@index([documentId])
}
```

Add a `chunks_rel KnowledgeChunk[]` relation to KnowledgeDocument (name it `chunksRel` to avoid conflict with existing `chunks` JSON field).

**Step 3: Create embeddings utility.**

Create `src/lib/knowledge/embeddings.ts`:

```typescript
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const EMBEDDING_MODEL = "text-embedding-3-small";
const EMBEDDING_DIMENSIONS = 1536;

export async function embedText(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: EMBEDDING_MODEL,
    input: text.slice(0, 8000), // Truncate to token limit safety
  });
  return response.data[0].embedding;
}

export async function embedBatch(texts: string[]): Promise<number[][]> {
  // OpenAI supports batching up to 2048 inputs
  const response = await openai.embeddings.create({
    model: EMBEDDING_MODEL,
    input: texts.map(t => t.slice(0, 8000)),
  });
  return response.data
    .sort((a, b) => a.index - b.index)
    .map(d => d.embedding);
}

export { EMBEDDING_MODEL, EMBEDDING_DIMENSIONS };
```

Check if `openai` is already in package.json. If not, run `npm install openai`.

Run `npx prisma validate` to confirm schema is valid, then `npx prisma db push` to apply changes (creates pgvector extension + KnowledgeChunk table), then `npx prisma generate`.
  </action>
  <verify>
    <automated>cd /Users/jjay/programs/outsignal-agents && npx prisma validate 2>&1 | head -5 && node -e "const { PrismaClient } = require('@prisma/client'); const p = new PrismaClient(); p.knowledgeChunk.count().then(c => { console.log('OK: KnowledgeChunk count =', c); p.\$disconnect(); }).catch(e => { console.error('FAIL:', e.message); p.\$disconnect(); })"</automated>
  </verify>
  <done>pgvector extension enabled in Neon, KnowledgeChunk model exists with embedding column, embeddings.ts utility created with embedText and embedBatch functions, Prisma client regenerated</done>
</task>

<task type="auto">
  <name>Task 2: Migrate knowledge base to pgvector and upgrade searchKnowledge</name>
  <files>src/lib/knowledge/store.ts</files>
  <action>
**Step 1: Update ingestDocument to create KnowledgeChunk records with embeddings.**

Modify `ingestDocument()` in store.ts:
- After creating the KnowledgeDocument record (keep existing behavior for backward compat)
- Generate embeddings for each chunk using `embedBatch()` from `./embeddings`
- Create KnowledgeChunk records with the embeddings using raw SQL (Prisma doesn't support vector type in create):
  ```typescript
  await prisma.$executeRaw`
    INSERT INTO "KnowledgeChunk" (id, "documentId", content, embedding, "chunkIndex", "createdAt")
    VALUES (${id}, ${docId}, ${content}, ${embedding}::vector, ${index}, NOW())
  `
  ```
- Use `cuid()` (import from `@paralleldrive/cuid2` or generate inline with `crypto.randomUUID()`) for chunk IDs

**Step 2: Upgrade searchKnowledge to use pgvector cosine similarity.**

Replace the keyword matching logic with a vector similarity query:
- Embed the query text using `embedText()`
- Run a raw SQL query for cosine similarity:
  ```typescript
  const results = await prisma.$queryRaw`
    SELECT kc.content, kc."documentId", kd.title, kd.tags,
           1 - (kc.embedding <=> ${queryEmbedding}::vector) as similarity
    FROM "KnowledgeChunk" kc
    JOIN "KnowledgeDocument" kd ON kd.id = kc."documentId"
    ${tags ? Prisma.sql`WHERE kd.tags LIKE ${'%' + tags + '%'}` : Prisma.empty}
    ORDER BY kc.embedding <=> ${queryEmbedding}::vector
    LIMIT ${limit}
  `
  ```
- Convert to the existing return format: `{ title, chunk: content, tags }`
- Keep the same function signature so callers don't break
- Add a fallback: if no KnowledgeChunk records exist yet (pre-migration), fall back to the old keyword matching. This ensures graceful degradation.

**Step 3: Add a re-embed migration function.**

Add `export async function reembedAllDocuments()` to store.ts:
- Fetch all KnowledgeDocument records
- For each, parse the `chunks` JSON field
- Delete existing KnowledgeChunk records for that document
- Generate embeddings in batches and insert as KnowledgeChunk records
- Log progress: `Re-embedded {docTitle}: {chunkCount} chunks`
- Return `{ documentsProcessed, chunksCreated }`

**Step 4: Create a one-time migration script.**

Create `scripts/reembed-knowledge.ts`:
```typescript
import { reembedAllDocuments } from "../src/lib/knowledge/store";

async function main() {
  console.log("Re-embedding all knowledge base documents...");
  const result = await reembedAllDocuments();
  console.log(`Done: ${result.documentsProcessed} docs, ${result.chunksCreated} chunks`);
  process.exit(0);
}
main().catch(e => { console.error(e); process.exit(1); });
```

Run the migration script to re-embed all 46+ documents:
```bash
npx tsx scripts/reembed-knowledge.ts
```

IMPORTANT: This requires OPENAI_API_KEY to be set in .env. If it's not present, check Vercel env vars or ask the user.
  </action>
  <verify>
    <automated>cd /Users/jjay/programs/outsignal-agents && node -e "const { PrismaClient } = require('@prisma/client'); const p = new PrismaClient(); p.knowledgeChunk.count().then(c => { console.log('KnowledgeChunk count:', c); if (c > 0) { console.log('OK: Documents re-embedded'); } else { console.log('WARN: No chunks found — migration may not have run'); } p.\$disconnect(); })"</automated>
  </verify>
  <done>searchKnowledge uses pgvector cosine similarity, all 46+ documents re-embedded into KnowledgeChunk table with vector embeddings, keyword matching retained as fallback for empty chunk table</done>
</task>

<task type="auto">
  <name>Task 3: Make searchKnowledgeBase a shared tool across agents</name>
  <files>src/lib/agents/writer.ts, src/lib/agents/orchestrator.ts, src/lib/agents/leads.ts</files>
  <action>
**Step 1: Extract the searchKnowledgeBase tool into a shared module.**

Create `src/lib/agents/shared-tools.ts`:
```typescript
import { tool } from "ai";
import { z } from "zod";
import { searchKnowledge } from "@/lib/knowledge/store";

export const searchKnowledgeBase = tool({
  description:
    "Search the Outsignal knowledge base for cold email and LinkedIn outreach best practices, frameworks, templates, and guidelines. Returns semantically relevant passages. Use this to ground content in proven strategies.",
  inputSchema: z.object({
    query: z.string().describe("Search query — e.g. 'subject line best practices', 'follow-up sequence', 'LinkedIn connection request'"),
    tags: z.string().optional().describe("Filter by tag — e.g. 'cold-email', 'linkedin', 'subject-lines'"),
    limit: z.number().optional().default(8).describe("Max results (default 8)"),
  }),
  execute: async ({ query, tags, limit }) => {
    const results = await searchKnowledge(query, { limit, tags });
    if (results.length === 0) {
      return {
        message: "No matching knowledge base entries found. Write based on your expertise.",
        results: [],
      };
    }
    return {
      message: `Found ${results.length} relevant passage(s).`,
      results: results.map((r) => ({
        source: r.title,
        content: r.chunk,
      })),
    };
  },
});
```

**Step 2: Update writer.ts to use shared tool.**
- Remove the inline `searchKnowledgeBase` tool definition from `writerTools`
- Import `searchKnowledgeBase` from `./shared-tools`
- Add it to the `writerTools` object: `searchKnowledgeBase` (spread or direct assignment)

**Step 3: Add searchKnowledgeBase to orchestrator tools.**
- Import `searchKnowledgeBase` from `./shared-tools`
- Add it to the `dashboardTools` or directly to `orchestratorTools` export

**Step 4: Add searchKnowledgeBase to leads agent tools.**
- Import `searchKnowledgeBase` from `./shared-tools`
- Add it to the leads agent tools (in the tool definition object in leads.ts)
- This gives the Leads Agent access to knowledge base for ICP-related queries

Do NOT change the `searchKnowledge` function interface or import path — only the tool wrapper is being extracted and shared.
  </action>
  <verify>
    <automated>cd /Users/jjay/programs/outsignal-agents && npx tsc --noEmit 2>&1 | head -20</automated>
  </verify>
  <done>searchKnowledgeBase tool is extracted to shared-tools.ts, imported and available in writer, leads, and orchestrator agents. TypeScript compiles without errors.</done>
</task>

</tasks>

<verification>
1. `npx prisma validate` passes
2. KnowledgeChunk table exists in database with records (> 0 after migration)
3. `searchKnowledge("cold email best practices")` returns semantically relevant results
4. searchKnowledgeBase tool exists in writer, leads, and orchestrator tool sets
5. `npx tsc --noEmit` passes with no errors
6. Old keyword matching still works as fallback when KnowledgeChunk table is empty
</verification>

<success_criteria>
Knowledge base search uses pgvector cosine similarity for semantic matching. All 46+ existing documents are re-embedded. searchKnowledgeBase is a shared tool available to writer, leads, and orchestrator agents.
</success_criteria>

<output>
After completion, create `.planning/phases/08-campaign-entity-writer/08-02-SUMMARY.md`
</output>
